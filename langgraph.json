{
  "name": "LangChainChatbot",
  "description": "A conversational chatbot using LangChain and LangGraph",
  "dependencies": [
    "langchain",
    "langgraph",
    "openai",
    "chromadb"
  ],
  "env": {
    "OPENAI_API_KEY": {
      "required": true
    }
  },
  "graphs": {
    "chatbot_graph": {
      // The node to start with
      "start": "start",

      "nodes": {
        "start": {
          "type": "input",
          "description": "Receives user input and sends it to processing",
          "next": "process_query"
        },

        "process_query": {
          "type": "llm",
          // Instead of a plain "gpt-4" or "gpt-4o",
          // reference the actual OpenAI LLM class:
          "model": "langchain.llms.openai:OpenAI",

          "description": "Processes user input with LangChain",
          "parameters": {
            // Now you can specify the OpenAI model name here
            "model_name": "gpt-4",
            "temperature": 0.7,
            "max_tokens": 500
          },
          "next": "store_memory"
        },

        "store_memory": {
          "type": "memory",
          "description": "Stores conversation history using a vector database",
          "parameters": {
            "storage": "chroma",
            "retrieval": "similarity",

            // Instead of a plain "text-embedding-ada-002",
            // reference the actual embedding class:
            "embedding_model": "langchain.embeddings.openai:OpenAIEmbeddings",

            // Then specify the actual embedding model name here:
            "embedding_model_params": {
              "model_name": "text-embedding-ada-002"
            }
          },
          "next": "output_response"
        },

        "output_response": {
          "type": "output",
          "description": "Outputs chatbot's response to the user"
        }
      },

      "edges": [
        { "from": "start",         "to": "process_query" },
        { "from": "process_query", "to": "store_memory" },
        { "from": "store_memory",  "to": "output_response" }
      ]
    }
  }
}